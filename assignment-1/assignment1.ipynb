{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the random Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1 -1  1  1 -1  1 -1  1  1 -1  1\n",
      " -1  1  1  1  1 -1 -1 -1  1  1 -1 -1  1 -1  1 -1  1 -1 -1 -1 -1  1  1  1\n",
      "  1 -1  1  1 -1 -1  1  1 -1  1 -1 -1 -1  1  1 -1  1 -1 -1 -1 -1  1  1  1\n",
      " -1 -1  1  1 -1 -1  1  1 -1  1 -1  1  1  1  1 -1 -1 -1  1  1 -1  1 -1 -1\n",
      "  1 -1 -1 -1]\n",
      "[[0.91473138 0.0650588  0.90545463 0.43170181 0.52135365 0.60782222\n",
      "  0.33897151 0.48266951 0.81108744 0.2931116 ]\n",
      " [0.26836888 0.50158593 0.67506033 0.18902072 0.58672815 0.75235711\n",
      "  0.74166072 0.61554244 0.53500966 0.68622984]\n",
      " [0.73340325 0.60494264 0.86859463 0.23673276 0.36851718 0.22172385\n",
      "  0.46184412 0.30289697 0.86241353 0.01536816]\n",
      " [0.553998   0.96629304 0.07847826 0.91835454 0.21326236 0.50735612\n",
      "  0.93592945 0.50347157 0.69831463 0.0999497 ]\n",
      " [0.82634161 0.91237282 0.77895022 0.3638494  0.28488727 0.0014513\n",
      "  0.12328917 0.1437361  0.76102399 0.80935997]\n",
      " [0.20602647 0.39765355 0.85582931 0.70943579 0.89609137 0.29166501\n",
      "  0.94477585 0.96633106 0.5959912  0.43769565]\n",
      " [0.39048886 0.9527274  0.85352867 0.61848213 0.6965844  0.77466434\n",
      "  0.45211457 0.82595632 0.97627195 0.53446458]\n",
      " [0.14604885 0.46097077 0.29430759 0.7130957  0.55072381 0.01198488\n",
      "  0.9249578  0.42377936 0.79793501 0.66000756]\n",
      " [0.65083965 0.43597598 0.75363256 0.04474089 0.79947007 0.52982705\n",
      "  0.51103995 0.86069638 0.79786663 0.63845591]\n",
      " [0.76215155 0.39600689 0.49561373 0.26874997 0.4053818  0.96167451\n",
      "  0.53131075 0.11841671 0.43216613 0.67112397]\n",
      " [0.04158669 0.8772492  0.42332791 0.56785326 0.42823659 0.48673297\n",
      "  0.70722956 0.8665883  0.58105527 0.99604967]\n",
      " [0.96175399 0.80007013 0.38720871 0.28510865 0.87864861 0.50728512\n",
      "  0.61355451 0.25001733 0.92090586 0.05780597]\n",
      " [0.73337782 0.45384612 0.7398359  0.81199758 0.49099965 0.06495868\n",
      "  0.54641193 0.53798839 0.45228428 0.57089813]\n",
      " [0.49305606 0.93757532 0.37830601 0.62690694 0.69830553 0.87159808\n",
      "  0.89686196 0.40808737 0.73027651 0.19110668]\n",
      " [0.37676914 0.30284068 0.49823877 0.99432446 0.41966572 0.81009201\n",
      "  0.10882557 0.95642925 0.1215196  0.32661878]\n",
      " [0.26469349 0.14800848 0.05334318 0.80723135 0.77612489 0.96586737\n",
      "  0.53242404 0.67777678 0.95249866 0.91090107]\n",
      " [0.8001756  0.85939892 0.1341881  0.31351408 0.45142299 0.83550805\n",
      "  0.16274863 0.98805783 0.60321379 0.83824586]\n",
      " [0.91055088 0.53722545 0.06310081 0.52100872 0.72296392 0.41868031\n",
      "  0.70079206 0.00320157 0.38790684 0.93800505]\n",
      " [0.4950919  0.15395112 0.36591919 0.52169602 0.54072675 0.64692139\n",
      "  0.6476836  0.38382316 0.68423078 0.57344611]\n",
      " [0.01362991 0.86557379 0.79091318 0.75048687 0.68669795 0.78939105\n",
      "  0.94975894 0.46986474 0.16615971 0.03367906]\n",
      " [0.27692017 0.83261129 0.16402266 0.55445836 0.65867245 0.78888192\n",
      "  0.63420266 0.09594359 0.18449836 0.15615684]\n",
      " [0.25223668 0.80126996 0.27049045 0.12384172 0.75531499 0.85727939\n",
      "  0.92901268 0.10482624 0.29571813 0.71483035]\n",
      " [0.69834407 0.76737479 0.68359564 0.14659118 0.95707449 0.6955303\n",
      "  0.78366023 0.29039515 0.08508607 0.83519074]\n",
      " [0.63732173 0.4005444  0.29879977 0.58144925 0.51650757 0.34184379\n",
      "  0.51113663 0.34685907 0.10948553 0.81687486]\n",
      " [0.21050368 0.03322109 0.84228969 0.37370994 0.40512601 0.10348711\n",
      "  0.00417517 0.32377164 0.90152163 0.89143048]\n",
      " [0.23561232 0.97351377 0.24342558 0.9185779  0.08017001 0.20058361\n",
      "  0.97088219 0.78047054 0.25137817 0.41612028]\n",
      " [0.63551567 0.484338   0.44120649 0.06853741 0.66272857 0.6265823\n",
      "  0.94048325 0.72472544 0.31401809 0.28174685]\n",
      " [0.79746367 0.46370877 0.15566343 0.64042125 0.13879219 0.89131121\n",
      "  0.71006474 0.30155514 0.01180282 0.12024906]\n",
      " [0.61231582 0.37983798 0.3514144  0.14682464 0.41787741 0.48467077\n",
      "  0.12755632 0.59115595 0.50023838 0.11078402]\n",
      " [0.79463657 0.66804469 0.26474288 0.82584283 0.45406484 0.71078743\n",
      "  0.56668957 0.49471201 0.47934203 0.15991293]\n",
      " [0.23453533 0.02176832 0.87712056 0.24435557 0.63789777 0.2615927\n",
      "  0.47972562 0.34376975 0.08275644 0.60724747]\n",
      " [0.19700505 0.12308813 0.19699934 0.17488527 0.31287626 0.66898501\n",
      "  0.73427812 0.07822764 0.17407666 0.10708296]\n",
      " [0.0714327  0.06239608 0.05807232 0.38057825 0.09284832 0.4997024\n",
      "  0.36281056 0.89013359 0.68711652 0.44313668]\n",
      " [0.06115625 0.61690234 0.54885449 0.38522054 0.69023948 0.10248663\n",
      "  0.00977674 0.95744593 0.44051139 0.14953468]\n",
      " [0.78425904 0.56230527 0.51279991 0.72487748 0.45319601 0.18116767\n",
      "  0.97782126 0.13996016 0.20832154 0.41269547]\n",
      " [0.3520087  0.23266711 0.44703248 0.63474286 0.05262871 0.81665484\n",
      "  0.42555917 0.89915243 0.4989936  0.47204996]\n",
      " [0.64314001 0.48823228 0.43517877 0.36792439 0.34460386 0.25046286\n",
      "  0.64115892 0.33802308 0.22879902 0.55509343]\n",
      " [0.93960083 0.2988278  0.18844606 0.85814232 0.87126841 0.21539076\n",
      "  0.18512933 0.05560372 0.30596851 0.16658899]\n",
      " [0.96412721 0.42333905 0.69521575 0.3916289  0.86882495 0.93286304\n",
      "  0.82721759 0.70540872 0.09509223 0.33599189]\n",
      " [0.4566412  0.90800822 0.57651786 0.2412013  0.72530653 0.16154611\n",
      "  0.84207565 0.91213922 0.04331419 0.42962961]\n",
      " [0.84991633 0.21106438 0.16725909 0.21603481 0.47657364 0.283455\n",
      "  0.46726071 0.50920788 0.11175331 0.92494688]\n",
      " [0.72582752 0.18449088 0.32266803 0.07771306 0.37180173 0.40382242\n",
      "  0.20137572 0.68601079 0.23570571 0.5255833 ]\n",
      " [0.23518226 0.42260107 0.53420708 0.85241205 0.81280472 0.12141602\n",
      "  0.73490589 0.04201399 0.495949   0.69877517]\n",
      " [0.36504363 0.4026396  0.90077588 0.39687441 0.94499373 0.76158165\n",
      "  0.6560102  0.7873197  0.14775369 0.77302905]\n",
      " [0.55921327 0.86210679 0.06525362 0.57615156 0.65975922 0.95043798\n",
      "  0.76060773 0.46428776 0.71828085 0.63886336]\n",
      " [0.12397117 0.5586562  0.40896876 0.02590002 0.47137055 0.29690372\n",
      "  0.63916789 0.49748358 0.02752881 0.78254534]\n",
      " [0.90853351 0.10311436 0.97090924 0.2417458  0.23242628 0.81282198\n",
      "  0.9730227  0.51945573 0.51380907 0.02376352]\n",
      " [0.80169228 0.9743727  0.10988014 0.03883444 0.07875543 0.929557\n",
      "  0.84267145 0.54635694 0.45182427 0.54571485]\n",
      " [0.83529377 0.34566153 0.70345539 0.72367123 0.83737133 0.33448313\n",
      "  0.31581846 0.23259123 0.74073758 0.66477639]\n",
      " [0.50655331 0.39578376 0.97453696 0.16306997 0.01950148 0.14713864\n",
      "  0.48292035 0.76202449 0.83720398 0.68599564]\n",
      " [0.41042407 0.10244383 0.60649681 0.94743438 0.01219442 0.03182307\n",
      "  0.72598224 0.81818022 0.82191019 0.62242117]\n",
      " [0.99692052 0.78508502 0.48521851 0.3298029  0.43948678 0.11103557\n",
      "  0.52601535 0.75858636 0.47658261 0.44429936]\n",
      " [0.27847254 0.26651906 0.94608822 0.62575107 0.46084709 0.58461127\n",
      "  0.76576465 0.5025726  0.59064446 0.48060019]\n",
      " [0.35256703 0.95813067 0.20523468 0.59798319 0.87044132 0.30482908\n",
      "  0.9432774  0.29845198 0.34675485 0.04968711]\n",
      " [0.5966183  0.87942783 0.20767871 0.91753675 0.38239511 0.98402579\n",
      "  0.89515224 0.72725241 0.91957112 0.58897726]\n",
      " [0.86413993 0.8915899  0.00641872 0.09259503 0.82314735 0.87796269\n",
      "  0.0358053  0.56690409 0.941331   0.73904642]\n",
      " [0.26179316 0.64809556 0.41243539 0.60921517 0.90525895 0.40439884\n",
      "  0.13632333 0.9902204  0.95974472 0.93180482]\n",
      " [0.94251968 0.98169935 0.59418056 0.33779315 0.02095294 0.59141737\n",
      "  0.88258466 0.42588214 0.23066404 0.72425355]\n",
      " [0.26302468 0.16522028 0.12833857 0.2455974  0.05338131 0.44393189\n",
      "  0.82690394 0.14280512 0.28490238 0.09144676]\n",
      " [0.84817605 0.05604428 0.68168061 0.24065073 0.57587849 0.26305541\n",
      "  0.29815921 0.2070079  0.34078578 0.42735642]\n",
      " [0.00698208 0.91032067 0.31366309 0.62225565 0.11964999 0.147962\n",
      "  0.50208049 0.89114008 0.98939678 0.70289727]\n",
      " [0.73618239 0.54332274 0.7723162  0.83020531 0.31431629 0.48056453\n",
      "  0.01670791 0.75790192 0.41677563 0.99967841]\n",
      " [0.36087372 0.00555213 0.77315213 0.46845028 0.43674788 0.22913072\n",
      "  0.42289347 0.290488   0.65238813 0.30697007]\n",
      " [0.37735926 0.4323678  0.12249542 0.72528294 0.91802746 0.11282955\n",
      "  0.11813407 0.60895885 0.68858558 0.53506342]\n",
      " [0.62602457 0.682656   0.31679566 0.98047613 0.49804282 0.74297115\n",
      "  0.20211738 0.77709219 0.88972945 0.515332  ]\n",
      " [0.50700139 0.95259786 0.77441987 0.37779248 0.872927   0.5156927\n",
      "  0.11389667 0.53340265 0.0412401  0.11843973]\n",
      " [0.02765581 0.575606   0.05767416 0.96854488 0.02598114 0.24380843\n",
      "  0.48034325 0.68232857 0.12478136 0.14088899]\n",
      " [0.24992333 0.58829476 0.04616323 0.10724031 0.9570521  0.33207046\n",
      "  0.99330333 0.65684436 0.51361697 0.28631258]\n",
      " [0.76260009 0.4279453  0.98134137 0.30628156 0.82652403 0.43336628\n",
      "  0.92422542 0.06076957 0.17976357 0.32094369]\n",
      " [0.01541101 0.01188852 0.23408603 0.38746904 0.25683889 0.9930431\n",
      "  0.45704048 0.89258778 0.01729303 0.2647186 ]\n",
      " [0.12645213 0.3756338  0.67734489 0.75498387 0.31383455 0.75098723\n",
      "  0.53163011 0.98502915 0.30221536 0.91755821]\n",
      " [0.31529317 0.6321679  0.52721772 0.97959633 0.64184194 0.627065\n",
      "  0.00827882 0.89578584 0.56562382 0.73330157]\n",
      " [0.12845307 0.75208146 0.28374624 0.83873447 0.0954844  0.43207931\n",
      "  0.98916014 0.0183563  0.15937267 0.01480017]\n",
      " [0.55754001 0.29703863 0.45370124 0.81165752 0.90461855 0.6380188\n",
      "  0.17666456 0.91810815 0.93999891 0.18679692]\n",
      " [0.57672915 0.42443033 0.53966496 0.11184652 0.94063774 0.82923958\n",
      "  0.21515787 0.33243161 0.69801996 0.15028906]\n",
      " [0.58147208 0.76095668 0.83653043 0.53956025 0.18611583 0.76290973\n",
      "  0.0239773  0.43534243 0.80548168 0.21672794]\n",
      " [0.90678731 0.53919943 0.73028468 0.72586085 0.71521178 0.31597683\n",
      "  0.75233043 0.88669917 0.93905108 0.83413609]\n",
      " [0.07493157 0.39571025 0.71614535 0.82139642 0.30940107 0.51403757\n",
      "  0.44093637 0.96007205 0.46017565 0.95365043]\n",
      " [0.51485865 0.96067726 0.07468752 0.88319595 0.2121029  0.5776826\n",
      "  0.65247631 0.45743123 0.10920028 0.81921421]\n",
      " [0.09257881 0.27147888 0.72876073 0.97516758 0.95621006 0.58685766\n",
      "  0.89732274 0.24373794 0.74564455 0.21712159]\n",
      " [0.28520471 0.64273318 0.73249783 0.34278449 0.24837971 0.15780194\n",
      "  0.51600069 0.83392169 0.32325472 0.9678743 ]\n",
      " [0.00715548 0.55077625 0.93339075 0.28366531 0.3350028  0.07154615\n",
      "  0.55648149 0.30907814 0.65487934 0.16145933]\n",
      " [0.92110641 0.31425006 0.99717682 0.88199334 0.21137974 0.12249689\n",
      "  0.3386468  0.91913265 0.60424494 0.18177284]\n",
      " [0.38356618 0.48903262 0.31065257 0.2773902  0.67565976 0.98843238\n",
      "  0.31294226 0.81146552 0.37346993 0.99837709]\n",
      " [0.56839748 0.92787859 0.14268671 0.34572312 0.45800873 0.75047986\n",
      "  0.24496035 0.41579208 0.87588863 0.514845  ]\n",
      " [0.72596243 0.30131768 0.83727079 0.09956803 0.10010397 0.95381391\n",
      "  0.03757819 0.15636045 0.11055692 0.24312958]\n",
      " [0.46670497 0.02766765 0.98635732 0.69709818 0.01249477 0.54812571\n",
      "  0.33660687 0.64640018 0.58982169 0.35276234]\n",
      " [0.3605531  0.04846395 0.11681452 0.17163781 0.42730314 0.02527222\n",
      "  0.52859048 0.88787852 0.17168098 0.0286088 ]\n",
      " [0.2990554  0.97103491 0.20399088 0.71491226 0.01711005 0.45911498\n",
      "  0.32687681 0.53845833 0.80655825 0.07051197]\n",
      " [0.46748438 0.56948491 0.85892738 0.95188143 0.78693707 0.9292629\n",
      "  0.96360035 0.6258308  0.77639412 0.48833761]\n",
      " [0.21115317 0.91251967 0.81080423 0.71674491 0.8351197  0.11776858\n",
      "  0.80845836 0.03655971 0.47850828 0.30633386]\n",
      " [0.10945647 0.16268592 0.73568306 0.31754968 0.4949784  0.26055046\n",
      "  0.97695353 0.85293915 0.76788052 0.34541134]\n",
      " [0.18055626 0.92586607 0.53516776 0.40567773 0.58025866 0.71218119\n",
      "  0.88182718 0.99937497 0.89847581 0.63973187]\n",
      " [0.26241217 0.84719361 0.33301506 0.18704173 0.47523161 0.80720444\n",
      "  0.54139881 0.25632723 0.12996823 0.73621782]\n",
      " [0.62594588 0.6661814  0.91538176 0.37680822 0.32116395 0.47165572\n",
      "  0.06981105 0.81787943 0.68132643 0.27466218]\n",
      " [0.9424168  0.26250845 0.6045728  0.98218733 0.15843016 0.35220507\n",
      "  0.3063831  0.65736826 0.07907977 0.22389968]\n",
      " [0.8355108  0.04037458 0.41145258 0.4275054  0.23931685 0.05355873\n",
      "  0.65316736 0.64193215 0.4885658  0.92373892]\n",
      " [0.77299507 0.63702153 0.91609345 0.32516282 0.26815782 0.30759697\n",
      "  0.86662438 0.59485671 0.86697085 0.5986866 ]\n",
      " [0.82087582 0.07883167 0.7243281  0.44667962 0.80667282 0.43185552\n",
      "  0.06071435 0.38506892 0.45046769 0.4377579 ]\n",
      " [0.44388977 0.2275139  0.88291263 0.38833576 0.74420177 0.39434945\n",
      "  0.18113065 0.65339695 0.56376391 0.08353501]]\n"
     ]
    }
   ],
   "source": [
    "# We first generate a random dataset with number of features (m = 10) and number of instances (n = 100)\n",
    "# We also generate a random label vector y \\in {-1,1}\n",
    "\n",
    "n = 100 # Number of instances\n",
    "m = 10  # Number of Features\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n) # n-dimensional vector\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m) # m-dimensional vector\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Logistic Loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += np.log(1 + np.exp(-y[i] * y_pred[i]))\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            g[k] += -1 * y[i] * X[i][k] / (1 + np.exp(y_pred[i] * y[i]))\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "    \n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.004483461380004883\n",
      "Function value = 122.274444023278\n",
      "Printing Gradient:\n",
      "[19.72058111 21.83922336 23.09055854 22.32111737 21.92809695 17.94364089\n",
      " 23.12795594 23.24772995 20.40605657 19.31804168]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Least Squares \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += (y[i] - y_pred[i]) ** 2\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            g[k] += 2 * (y_pred[i] - y[i]) * X[i][k]\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "        \n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.002254486083984375\n",
      "Function value = 623.0566842351129\n",
      "Printing Gradient:\n",
      "[224.28352049 248.06363597 238.69580447 236.65993001 234.35444649\n",
      " 228.31882478 254.21298782 262.03529849 231.68054094 228.04989163]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Hinge Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += max(0, 1 - y[i] * y_pred[i])\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            if y_pred[i]*y[i] <= 1:\n",
    "                g[k] += -1 * y[i] * X[i][k]\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "    \n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.0007905960083007812\n",
      "Function value = 161.9737114674038\n",
      "Printing Gradient:\n",
      "[24.7473981  26.60377429 28.52150374 27.55426854 27.0813504  22.41112162\n",
      " 28.50749887 28.44352916 25.40888073 23.55496323]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90441/1203843815.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  f += np.log(1 + np.exp(-y[i] * y_pred[i]))\n",
      "/tmp/ipykernel_90441/1203843815.py:21: RuntimeWarning: overflow encountered in exp\n",
      "  g[k] += -1 * y[i] * X[i][k] / (1 + np.exp(y_pred[i] * y[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Loss\n",
      "Time Taken = 3.4032998085021973\n",
      "Function value = [inf]\n",
      "Printing Gradient:\n",
      "[24.91565548 25.48367699 26.50079266 ... 23.93407991 24.09286659\n",
      " 24.81819486]\n",
      "Least Square\n",
      "Time Taken = 2.146094560623169\n",
      "Function value = [6.4104154e+08]\n",
      "Printing Gradient:\n",
      "[262635.84102936 262042.14901381 257794.55571874 ... 246854.21417646\n",
      " 260135.42399998 271424.67119628]\n",
      "Hinge Loss\n",
      "Time Taken = 2.1042685508728027\n",
      "Function value = [123291.48085078]\n",
      "Printing Gradient:\n",
      "[24.91565548 25.48367699 26.50079266 ... 23.93407991 24.09286659\n",
      " 24.81819486]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Logistic Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Least Square\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Hinge Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a vectorized version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.sum(np.log(1 + np.exp(-y_pred * y))) + lam * np.linalg.norm(w) / 2\n",
    "    g = - X.T @ (y / (1 + np.exp(y * y_pred))) + lam * w\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.sum((y_pred - y) ** 2) + lam * np.linalg.norm(w) / 2\n",
    "    g = 2 * X.T @ (y_pred - y) + lam * w\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.sum((1 >= y_pred * y) * (1 - y_pred*y)) + lam * np.linalg.norm(w) / 2\n",
    "    g = - X.T @ ((1 >= y_pred * y) * y) + lam * w\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Loss\n",
      "Time Taken = 0.013580322265625\n",
      "Function value = inf\n",
      "Printing Gradient:\n",
      "[[ 0.11293931  0.11293931 55.37390849 ...  0.11293931 55.37390849\n",
      "  55.37390849]\n",
      " [ 0.15236677  0.15236677 51.32776752 ...  0.15236677 51.32776752\n",
      "  51.32776752]\n",
      " [ 0.13967895  0.13967895 52.65474957 ...  0.13967895 52.65474957\n",
      "  52.65474957]\n",
      " ...\n",
      " [ 0.07231739  0.07231739 47.94471807 ...  0.07231739 47.94471807\n",
      "  47.94471807]\n",
      " [ 0.17878555  0.17878555 46.34448554 ...  0.17878555 46.34448554\n",
      "  46.34448554]\n",
      " [ 0.32927288  0.32927288 50.57995194 ...  0.32927288 50.57995194\n",
      "  50.57995194]]\n",
      "Least Square\n",
      "Time Taken = 0.010885238647460938\n",
      "Function value = 62308794146.3032\n",
      "Printing Gradient:\n",
      "[[275753.57521969 275753.57521969 275974.61909643 ... 275753.57521969\n",
      "  275974.61909643 275974.61909643]\n",
      " [255328.39499895 255328.39499895 255533.09660197 ... 255328.39499895\n",
      "  255533.09660197 255533.09660197]\n",
      " [262161.56248494 262161.56248494 262371.62276742 ... 262161.56248494\n",
      "  262371.62276742 262371.62276742]\n",
      " ...\n",
      " [238877.39398008 238877.39398008 239068.88358281 ... 238877.39398008\n",
      "  239068.88358281 239068.88358281]\n",
      " [230385.63418347 230385.63418347 230570.29698344 ... 230385.63418347\n",
      "  230570.29698344 230570.29698344]\n",
      " [250784.19378725 250784.19378725 250985.19650348 ... 250784.19378725\n",
      "  250985.19650348 250985.19650348]]\n",
      "Hinge Loss\n",
      "Time Taken = 0.007597208023071289\n",
      "Function value = 13733584.30855113\n",
      "Printing Gradient:\n",
      "[[ 0.11293931  0.11293931 55.37390849 ...  0.11293931 55.37390849\n",
      "  55.37390849]\n",
      " [ 0.15236677  0.15236677 51.32776752 ...  0.15236677 51.32776752\n",
      "  51.32776752]\n",
      " [ 0.13967895  0.13967895 52.65474957 ...  0.13967895 52.65474957\n",
      "  52.65474957]\n",
      " ...\n",
      " [ 0.07231739  0.07231739 47.94471807 ...  0.07231739 47.94471807\n",
      "  47.94471807]\n",
      " [ 0.17878555  0.17878555 46.34448554 ...  0.17878555 46.34448554\n",
      "  46.34448554]\n",
      " [ 0.32927288  0.32927288 50.57995194 ...  0.32927288 50.57995194\n",
      "  50.57995194]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90441/1406895302.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  f = np.sum(np.log(1 + np.exp(-y_pred * y))) + lam * np.linalg.norm(w) / 2\n",
      "/tmp/ipykernel_90441/1406895302.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  g = - X.T @ (y / (1 + np.exp(y * y_pred))) + lam * w\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Logistic Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LeastSquaresVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Least Square\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = HingeLossVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Hinge Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets us code the above Loss Fuctions in CVXPY!\n",
    "\n",
    "CVXPY is an open source Python-embedded modeling language for convex optimization problems. Link: https://www.cvxpy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    expression = cp.sum(cp.log(1 + cp.exp(cp.multiply(-y, X @ w)))) + lam * cp.norm(w, 2) / 2\n",
    "    Problem = cp.Problem(cp.Minimize(expression))\n",
    "    f = expression.value\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    expression = cp.sum_squares(X @ w - y) + lam * cp.norm(w, 2) / 2\n",
    "    Problem = cp.Problem(cp.Minimize(expression))\n",
    "    f = expression.value\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    expression = cp.sum(cp.pos(1 - cp.multiply(y, X @ w))) + lam * cp.norm(w, 2) / 2\n",
    "    Problem = cp.Problem(cp.Minimize(expression))\n",
    "    f = expression.value\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f1,g1] = LogisticLossCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value Naive = \" + str(f1))\n",
    "print(\"Printing Gradient Naive:\")\n",
    "print(g1)\n",
    "\n",
    "start = time.time()\n",
    "[f2,g2] = LeastSquaresCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value For = \" + str(f2))\n",
    "print(\"Printing Gradient For:\")\n",
    "print(g2)\n",
    "\n",
    "start = time.time()\n",
    "[f2,g2] = HingeLossCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value For = \" + str(f2))\n",
    "print(\"Printing Gradient For:\")\n",
    "print(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the losses with Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossFun(w, X, y, lam):\n",
    "    return error_ll\n",
    "\n",
    "def LeastSquaresFun(w, X, y, lam):\n",
    "    return error_ls\n",
    "\n",
    "def HingeLossFun(w, X, y, lam):\n",
    "    return error_hl\n",
    "\n",
    "def plot_errors(error_ll, error_ls, error_hl, num):\n",
    "    plt.plot(num, error_ll, label=\"Logistic Loss\")\n",
    "    plt.plot(num, error_ls, label=\"Least Squares\")\n",
    "    plt.plot(num, error_hl, label=\"Hinge Loss\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "error_ll = LogisticLossFun(w,X,y,1)\n",
    "error_ls = LeastSquaresFun(w,X,y,1)\n",
    "error_hl = HingeLossFun(w,X,y,1)\n",
    "plot_errors(error_ll, error_ls, error_hl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
