{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the random Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1  1 -1  1 -1 -1  1 -1  1 -1 -1 -1  1 -1\n",
      " -1  1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1 -1 -1  1 -1  1 -1 -1  1  1  1\n",
      "  1 -1 -1  1 -1 -1 -1  1  1 -1 -1  1 -1  1  1 -1  1 -1  1  1  1  1 -1 -1\n",
      " -1  1 -1 -1  1  1  1 -1 -1  1  1 -1 -1  1 -1  1 -1 -1  1  1  1 -1 -1 -1\n",
      " -1 -1  1  1]\n",
      "[[0.03464071 0.5596133  0.80104921 0.11227437 0.67873981 0.24390919\n",
      "  0.74389977 0.77176574 0.0469     0.69355279]\n",
      " [0.10514694 0.48771796 0.60912142 0.74569407 0.2249291  0.24216052\n",
      "  0.35572776 0.73829658 0.74767324 0.63374237]\n",
      " [0.53761657 0.48154364 0.8454833  0.28321559 0.30278682 0.41910136\n",
      "  0.57233946 0.54014636 0.27344629 0.30513814]\n",
      " [0.2437397  0.48836195 0.26845281 0.85772027 0.77281827 0.40305602\n",
      "  0.9900017  0.78947151 0.38301982 0.31218903]\n",
      " [0.3890894  0.99608747 0.94081578 0.30821005 0.37089578 0.92860704\n",
      "  0.84374602 0.24163896 0.42747421 0.79130684]\n",
      " [0.1580376  0.69377947 0.08193603 0.38738429 0.59409362 0.62704508\n",
      "  0.58639908 0.1702625  0.9532205  0.55068155]\n",
      " [0.97090199 0.12304835 0.90172792 0.11835322 0.32830563 0.5089266\n",
      "  0.84004788 0.92693591 0.98836727 0.89647842]\n",
      " [0.28960341 0.36701085 0.75857508 0.61057769 0.0055229  0.61431979\n",
      "  0.32610417 0.46238792 0.38147488 0.20546755]\n",
      " [0.38129807 0.10485169 0.16684119 0.50499288 0.46176592 0.99684629\n",
      "  0.44526028 0.58904557 0.98223616 0.22727063]\n",
      " [0.36418504 0.41132746 0.30997058 0.54695087 0.06088413 0.20435654\n",
      "  0.85613865 0.0735678  0.67939501 0.94719371]\n",
      " [0.9101867  0.8057305  0.17360303 0.6809541  0.09101488 0.14210727\n",
      "  0.2103589  0.29922554 0.6405167  0.07465684]\n",
      " [0.72096    0.8479641  0.49543953 0.61596758 0.86017481 0.89515439\n",
      "  0.31355871 0.04300833 0.52159596 0.25645592]\n",
      " [0.5003845  0.7162819  0.67277216 0.75697688 0.05275712 0.61126095\n",
      "  0.58210607 0.96977464 0.29422092 0.65204972]\n",
      " [0.83099537 0.04212803 0.14753377 0.16786324 0.05033571 0.23933432\n",
      "  0.23501363 0.84373192 0.04987201 0.25958596]\n",
      " [0.37224086 0.92844298 0.12132633 0.85183576 0.26927293 0.68514084\n",
      "  0.94748001 0.80433937 0.65082348 0.827996  ]\n",
      " [0.97085702 0.92834041 0.27520994 0.305835   0.91173004 0.11869704\n",
      "  0.20244307 0.14888872 0.83798194 0.15235212]\n",
      " [0.50522255 0.0193986  0.84353906 0.05311447 0.83806657 0.10021178\n",
      "  0.99591502 0.67938323 0.42701321 0.24679325]\n",
      " [0.22971467 0.34938051 0.06216422 0.36992255 0.89678245 0.35787848\n",
      "  0.42520231 0.81818858 0.87010474 0.29701066]\n",
      " [0.95805367 0.62326687 0.4459236  0.68532522 0.9425189  0.8579424\n",
      "  0.99604228 0.60538349 0.29779788 0.85878868]\n",
      " [0.02370662 0.08301283 0.93561363 0.57925483 0.85070337 0.80194148\n",
      "  0.86874412 0.44848424 0.77978199 0.93481484]\n",
      " [0.45527862 0.39401777 0.0078457  0.57536707 0.31447792 0.72926901\n",
      "  0.86886126 0.23073029 0.8646578  0.78323886]\n",
      " [0.35734573 0.79911112 0.54169314 0.90751946 0.93505028 0.69545514\n",
      "  0.77814086 0.33037887 0.82073828 0.33253175]\n",
      " [0.47080126 0.0333342  0.07727557 0.24275235 0.4714385  0.58492172\n",
      "  0.53238576 0.05083416 0.04054169 0.20414999]\n",
      " [0.07154333 0.56932474 0.24309834 0.66619616 0.97719485 0.70854034\n",
      "  0.0011212  0.22132085 0.62203974 0.40726117]\n",
      " [0.0422762  0.11686349 0.23286923 0.83329744 0.48192733 0.1638143\n",
      "  0.30814487 0.50541653 0.69694681 0.24037442]\n",
      " [0.94883504 0.6722803  0.88662192 0.28957359 0.30021102 0.18488991\n",
      "  0.1204798  0.91303714 0.11854053 0.8373925 ]\n",
      " [0.40143899 0.41136325 0.82661275 0.52973974 0.15761293 0.71879453\n",
      "  0.03904623 0.1545653  0.01306781 0.66732036]\n",
      " [0.94216805 0.35640671 0.12992715 0.35800751 0.15475445 0.44157383\n",
      "  0.20173902 0.99707186 0.57671372 0.45469957]\n",
      " [0.1111133  0.75880948 0.53096582 0.66853568 0.26736376 0.59033564\n",
      "  0.79638995 0.24873335 0.97708525 0.92869328]\n",
      " [0.0259088  0.32573715 0.53313865 0.12694728 0.17703685 0.20645165\n",
      "  0.27897665 0.83980294 0.58060035 0.26844585]\n",
      " [0.93038464 0.03982341 0.74786034 0.81005375 0.02234913 0.61037342\n",
      "  0.96288967 0.81019342 0.58110539 0.13488559]\n",
      " [0.82313325 0.08080688 0.5312625  0.19399979 0.67877056 0.60147238\n",
      "  0.49442125 0.16228586 0.36993861 0.57087365]\n",
      " [0.67315706 0.49524437 0.64439405 0.72520879 0.69875421 0.54817125\n",
      "  0.51660447 0.47598326 0.73108597 0.09958348]\n",
      " [0.65223319 0.3393601  0.29595036 0.80775101 0.58264705 0.43473815\n",
      "  0.18058274 0.66070287 0.14653002 0.65944338]\n",
      " [0.03435707 0.95951501 0.35253873 0.52076213 0.62307918 0.8931106\n",
      "  0.36095802 0.70609444 0.76543872 0.87402774]\n",
      " [0.94476247 0.03477942 0.71904626 0.23851003 0.21869897 0.67222438\n",
      "  0.54065823 0.54004129 0.18191737 0.52407058]\n",
      " [0.65196069 0.47287918 0.8625639  0.15622497 0.7887148  0.47250988\n",
      "  0.69288783 0.1007236  0.5440343  0.76140807]\n",
      " [0.62603667 0.94989026 0.35552406 0.35844278 0.22493723 0.67786616\n",
      "  0.82591038 0.10250927 0.7413726  0.86974352]\n",
      " [0.17900378 0.86072239 0.72169607 0.91107949 0.01338867 0.84900458\n",
      "  0.04621217 0.34402487 0.74371332 0.74599963]\n",
      " [0.24698285 0.25697838 0.02385764 0.28255564 0.57158112 0.16188901\n",
      "  0.07624808 0.70428234 0.3412515  0.72687535]\n",
      " [0.12036002 0.42873818 0.88058574 0.79635435 0.94248269 0.69400837\n",
      "  0.90373036 0.13357394 0.91346775 0.37295545]\n",
      " [0.00296119 0.54517707 0.04016072 0.22315588 0.59314089 0.62532745\n",
      "  0.30548529 0.81453902 0.2750614  0.62047942]\n",
      " [0.60968099 0.38648305 0.71510254 0.48579252 0.2071128  0.43500294\n",
      "  0.23477334 0.98922393 0.6007269  0.31659187]\n",
      " [0.63991027 0.82981178 0.19180772 0.585719   0.40930323 0.61993906\n",
      "  0.32475321 0.49632113 0.19555475 0.20949499]\n",
      " [0.2223726  0.14420572 0.14961155 0.88203731 0.51283441 0.57558913\n",
      "  0.38479701 0.187644   0.43620638 0.95801909]\n",
      " [0.15794077 0.34963724 0.40259416 0.10543061 0.12804618 0.05108104\n",
      "  0.44837617 0.01473332 0.024193   0.8181504 ]\n",
      " [0.41975566 0.35492062 0.52136821 0.04450521 0.32234049 0.9433939\n",
      "  0.61975713 0.60129146 0.66413471 0.10341968]\n",
      " [0.95719543 0.13927692 0.69095644 0.89250162 0.35523921 0.87806056\n",
      "  0.42733096 0.95793776 0.26870351 0.30722804]\n",
      " [0.94112327 0.07049423 0.17321333 0.21193263 0.04118029 0.2277164\n",
      "  0.90684287 0.35244191 0.67177297 0.79022855]\n",
      " [0.33171167 0.68639172 0.54976909 0.74846387 0.83260001 0.94404022\n",
      "  0.94274635 0.32445984 0.80883578 0.62451799]\n",
      " [0.20675504 0.41238546 0.19432402 0.07855373 0.44898794 0.90392893\n",
      "  0.75504332 0.97109197 0.56548097 0.10942755]\n",
      " [0.21677214 0.08779843 0.2885601  0.26209928 0.02208363 0.01993389\n",
      "  0.46114047 0.20932127 0.47325897 0.81510525]\n",
      " [0.87153445 0.44041075 0.52529829 0.17085779 0.32032967 0.13727455\n",
      "  0.21651651 0.19927525 0.25725978 0.48892666]\n",
      " [0.02381162 0.8871823  0.10543851 0.12402672 0.44186431 0.86698894\n",
      "  0.03162749 0.25985809 0.1682077  0.4819063 ]\n",
      " [0.22257592 0.61208921 0.80750161 0.36328493 0.53924119 0.22813207\n",
      "  0.96022956 0.7692629  0.28778516 0.62839614]\n",
      " [0.26476643 0.57878932 0.67427281 0.61283384 0.36973099 0.48374616\n",
      "  0.57112299 0.68382995 0.58547847 0.2607217 ]\n",
      " [0.87577626 0.71876132 0.64721896 0.19866792 0.57082788 0.48619227\n",
      "  0.29414674 0.99338242 0.60350101 0.43854079]\n",
      " [0.34929222 0.73304263 0.17724657 0.56228468 0.58480461 0.15272682\n",
      "  0.3632304  0.51057798 0.48741231 0.25312017]\n",
      " [0.79725683 0.17076681 0.04830547 0.54585168 0.01303111 0.26943318\n",
      "  0.65503398 0.74923346 0.94323964 0.14976805]\n",
      " [0.97446409 0.59215625 0.9835787  0.32750363 0.53322471 0.72143133\n",
      "  0.01238748 0.06866982 0.94177951 0.80642353]\n",
      " [0.12003609 0.30690867 0.05667895 0.24352649 0.46442779 0.07241077\n",
      "  0.11768603 0.36857142 0.88920527 0.94442514]\n",
      " [0.11113124 0.09436342 0.87242638 0.07432843 0.88035657 0.11824751\n",
      "  0.35948165 0.63887194 0.3882587  0.51504869]\n",
      " [0.63974601 0.50113184 0.23807694 0.72049802 0.09599454 0.89644002\n",
      "  0.16978678 0.94993925 0.66483712 0.82062289]\n",
      " [0.70907098 0.598884   0.81115688 0.53681937 0.69248659 0.59791219\n",
      "  0.60254851 0.95260797 0.50177216 0.94898205]\n",
      " [0.34293635 0.32087487 0.14173077 0.70283501 0.90785793 0.94550614\n",
      "  0.86011159 0.58860081 0.3655024  0.33722185]\n",
      " [0.12942007 0.30655545 0.82230495 0.06814818 0.6922082  0.22240926\n",
      "  0.5525577  0.59352474 0.0296535  0.60292954]\n",
      " [0.25877302 0.03359579 0.20821155 0.36008975 0.24975769 0.78827048\n",
      "  0.62814982 0.75533027 0.79548828 0.68684227]\n",
      " [0.82017113 0.00582676 0.89929728 0.78485315 0.98307698 0.93780802\n",
      "  0.96713689 0.70054759 0.92462108 0.23210467]\n",
      " [0.22510244 0.07753354 0.82976336 0.4703455  0.61952436 0.20275716\n",
      "  0.89526444 0.14667074 0.09713547 0.12080413]\n",
      " [0.62701027 0.86286442 0.49129674 0.79906338 0.64445002 0.37640593\n",
      "  0.82035768 0.39714212 0.49587143 0.28165138]\n",
      " [0.05646526 0.91264367 0.58086604 0.17764118 0.25240168 0.7403126\n",
      "  0.41622851 0.24274884 0.49173337 0.10019431]\n",
      " [0.45616995 0.75831998 0.13399255 0.86581827 0.27182103 0.25265442\n",
      "  0.89527643 0.26283266 0.5803246  0.20364365]\n",
      " [0.23121524 0.41569651 0.35212202 0.42312306 0.13490721 0.01129745\n",
      "  0.61209813 0.44056741 0.39924408 0.11863979]\n",
      " [0.94166937 0.95857371 0.1062544  0.04357177 0.49348125 0.45154047\n",
      "  0.67352421 0.7668857  0.74197248 0.91727697]\n",
      " [0.73787991 0.98094854 0.38025618 0.09794675 0.94445711 0.58699884\n",
      "  0.03986908 0.76758156 0.33153148 0.8276224 ]\n",
      " [0.45848878 0.249977   0.18830109 0.6790594  0.10485212 0.78752972\n",
      "  0.9706564  0.28188554 0.17474085 0.562924  ]\n",
      " [0.19508166 0.43933549 0.08947175 0.50101276 0.02529159 0.95150184\n",
      "  0.99608058 0.01578049 0.01744243 0.61036884]\n",
      " [0.82500855 0.71150838 0.83239043 0.22589821 0.1735749  0.39003838\n",
      "  0.26785172 0.46431964 0.19440979 0.09869137]\n",
      " [0.64424622 0.20395374 0.99925729 0.62029763 0.13276349 0.56299554\n",
      "  0.85596024 0.82997882 0.96882096 0.96632593]\n",
      " [0.11806034 0.06861275 0.00968812 0.46198393 0.88902422 0.5331714\n",
      "  0.01906018 0.27152701 0.17108041 0.5375008 ]\n",
      " [0.7318552  0.63943659 0.91902325 0.9531214  0.98008199 0.95070875\n",
      "  0.17449137 0.90125156 0.57766673 0.38161243]\n",
      " [0.69440832 0.32799276 0.12216159 0.59578567 0.48450348 0.35327984\n",
      "  0.08148018 0.09596143 0.63265359 0.23260082]\n",
      " [0.65050562 0.81429796 0.39036633 0.27767313 0.17564508 0.41771436\n",
      "  0.45283805 0.91491689 0.75914707 0.29446411]\n",
      " [0.21984287 0.3410218  0.46592452 0.49182501 0.59989363 0.30663306\n",
      "  0.85951723 0.7257993  0.21254635 0.39714262]\n",
      " [0.70130774 0.24825378 0.39004194 0.16041872 0.95955596 0.75500142\n",
      "  0.50830771 0.45156639 0.84403458 0.51879006]\n",
      " [0.91237371 0.9059563  0.67995076 0.49785753 0.37652643 0.85660464\n",
      "  0.25882111 0.04807322 0.64050848 0.56119806]\n",
      " [0.69308814 0.28161417 0.13511655 0.54737337 0.17872386 0.74506701\n",
      "  0.4718622  0.07805439 0.46808104 0.0541507 ]\n",
      " [0.82870522 0.68803372 0.49231798 0.1245882  0.86390902 0.80327262\n",
      "  0.00434875 0.38676875 0.7316848  0.47795551]\n",
      " [0.97485682 0.16424702 0.15924081 0.84303055 0.13012411 0.69508788\n",
      "  0.69095364 0.0148692  0.48791145 0.5419136 ]\n",
      " [0.14309649 0.25687948 0.99214547 0.86494548 0.01355633 0.26235389\n",
      "  0.54237216 0.93189714 0.79470218 0.75849537]\n",
      " [0.14711794 0.74535828 0.90541389 0.09634185 0.06714167 0.34327732\n",
      "  0.0339887  0.76938023 0.66516954 0.32532865]\n",
      " [0.49553199 0.15738818 0.9694234  0.48291779 0.03493281 0.9841772\n",
      "  0.93455572 0.12168583 0.76525802 0.2576003 ]\n",
      " [0.8091761  0.81529884 0.37688869 0.52034914 0.14161153 0.46179703\n",
      "  0.82814868 0.76281478 0.1057005  0.41196664]\n",
      " [0.99265914 0.0053206  0.1910656  0.16575323 0.56016621 0.03336019\n",
      "  0.94207499 0.77372046 0.73626361 0.55337795]\n",
      " [0.72987941 0.72763762 0.17177586 0.74883058 0.00708161 0.05266373\n",
      "  0.23935182 0.42210913 0.07462535 0.83135389]\n",
      " [0.28923147 0.68280027 0.25023967 0.57705829 0.65256363 0.82900173\n",
      "  0.09245723 0.16464392 0.03483988 0.90028258]\n",
      " [0.40435346 0.97715137 0.21917694 0.11155875 0.89320296 0.79684845\n",
      "  0.46586042 0.41019083 0.73684328 0.96377465]\n",
      " [0.39101001 0.58350913 0.87304016 0.52676782 0.15150136 0.74150561\n",
      "  0.46922312 0.91384689 0.1680881  0.39580812]\n",
      " [0.25491533 0.80091843 0.62388764 0.91766406 0.34412859 0.17705122\n",
      "  0.85431037 0.40925263 0.95327322 0.10696855]\n",
      " [0.01528527 0.51052972 0.77093096 0.83579359 0.06030199 0.17481224\n",
      "  0.65723073 0.82868633 0.9051983  0.09670647]]\n"
     ]
    }
   ],
   "source": [
    "# We first generate a random dataset with number of features (m = 10) and number of instances (n = 100)\n",
    "# We also generate a random label vector y \\in {-1,1}\n",
    "\n",
    "n = 100 # Number of instances\n",
    "m = 10  # Number of Features \n",
    "\n",
    "X = np.random.rand(n,m) \n",
    "y = np.random.rand(n) # n-dimensional vector\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1) # m-dimensional vector\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Logistic Loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += np.log(1 + np.exp(-y[i] * y_pred[i]))\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            g[k] += -1 * y[i] * X[i][k] / (1 + np.exp(y_pred[i] * y[i]))\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "    \n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.0058286190032958984\n",
      "Function value = [148.47457196]\n",
      "Printing Gradient:\n",
      "[20.99227956 23.60082748 20.27172267 24.89235895 21.80355854 25.90878363\n",
      " 23.42307349 22.70935056 25.27119556 26.05699708]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Least Squares \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += (y[i] - y_pred[i]) ** 2\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            g[k] += 2 * (y_pred[i] - y[i]) * X[i][k]\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "        \n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 0.0027070045471191406\n",
      "Function value = [824.90240215]\n",
      "Printing Gradient:\n",
      "[257.4672939  264.68828218 253.29646101 268.65462663 246.57578303\n",
      " 295.96194524 274.58732448 266.3442941  288.37065213 282.44703588]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Hinge Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += max(0, 1 - y[i] * y_pred[i])\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            if y_pred[i]*y[i] <= 1:\n",
    "                g[k] += -1 * y[i] * X[i][k]\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "    \n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 2.1474955081939697\n",
      "Function value = [138367.64287142]\n",
      "Printing Gradient:\n",
      "[30.19117072 26.20545841 24.32432151 ... 27.32422749 32.55625335\n",
      " 26.52044547]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66106/1203843815.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  f += np.log(1 + np.exp(-y[i] * y_pred[i]))\n",
      "/tmp/ipykernel_66106/1203843815.py:21: RuntimeWarning: overflow encountered in exp\n",
      "  g[k] += -1 * y[i] * X[i][k] / (1 + np.exp(y_pred[i] * y[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Loss\n",
      "Time Taken = 3.445894718170166\n",
      "Function value = [inf]\n",
      "Printing Gradient:\n",
      "[24.97999543 29.78695387 23.55741636 ... 24.72331878 25.67714269\n",
      " 25.28994668]\n",
      "Least Square\n",
      "Time Taken = 2.1516599655151367\n",
      "Function value = [6.13284004e+08]\n",
      "Printing Gradient:\n",
      "[247737.12050586 273182.89237057 234231.6264131  ... 249177.673977\n",
      " 247526.57480356 257983.57958173]\n",
      "Hinge Loss\n",
      "Time Taken = 2.102675437927246\n",
      "Function value = [125493.21623385]\n",
      "Printing Gradient:\n",
      "[24.97999543 29.78695387 23.55741636 ... 24.72331878 25.67714269\n",
      " 25.28994668]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Logistic Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Least Square\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Hinge Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a vectorized version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.sum(np.log(1 + np.exp(-y_pred * y))) + lam * np.linalg.norm(w) / 2\n",
    "    g = - X.T @ (y / (1 + np.exp(y * y_pred))) + lam * w\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.linalg.norm(y_pred - y) + lam * np.linalg.norm(w) / 2\n",
    "    g = 2 * X.T @ (y_pred - y) + lam * w\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.sum((1 >= y_pred * y) * (1 - y_pred*y)) + lam * np.linalg.norm(w) / 2\n",
    "    g = - X.T @ ((1 >= y_pred * y) * y) + lam * w\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Loss\n",
      "Time Taken = 0.011443853378295898\n",
      "Function value = inf\n",
      "Printing Gradient:\n",
      "[[ 0.61002058 51.91568763  0.61002058 ...  0.61002058  0.61002058\n",
      "  51.91568763]\n",
      " [ 0.44569461 55.35143704  0.44569461 ...  0.44569461  0.44569461\n",
      "  55.35143704]\n",
      " [ 0.75720532 44.1527623   0.75720532 ...  0.75720532  0.75720532\n",
      "  44.1527623 ]\n",
      " ...\n",
      " [ 0.1681764  46.52332579  0.1681764  ...  0.1681764   0.1681764\n",
      "  46.52332579]\n",
      " [ 0.42350058 51.4038948   0.42350058 ...  0.42350058  0.42350058\n",
      "  51.4038948 ]\n",
      " [ 0.56448345 48.80009498  0.56448345 ...  0.56448345  0.56448345\n",
      "  48.80009498]]\n",
      "Least Square\n",
      "Time Taken = 0.010295867919921875\n",
      "Function value = 249996.4678035193\n",
      "Printing Gradient:\n",
      "[[256271.37863064 256476.60129884 256271.37863064 ... 256271.37863064\n",
      "  256271.37863064 256476.60129884]\n",
      " [274402.18268715 274621.80565688 274402.18268715 ... 274402.18268715\n",
      "  274402.18268715 274621.80565688]\n",
      " [216910.20202959 217083.78425753 216910.20202959 ... 216910.20202959\n",
      "  216910.20202959 217083.78425753]\n",
      " ...\n",
      " [231637.97323239 231823.39382994 231637.97323239 ... 231637.97323239\n",
      "  231637.97323239 231823.39382994]\n",
      " [254839.69660328 255043.61818018 254839.69660328 ... 254839.69660328\n",
      "  254839.69660328 255043.61818018]\n",
      " [240983.99388324 241176.93632935 240983.99388324 ... 240983.99388324\n",
      "  240983.99388324 241176.93632935]]\n",
      "Hinge Loss\n",
      "Time Taken = 0.014481067657470703\n",
      "Function value = 12003267.345550464\n",
      "Printing Gradient:\n",
      "[[ 0.61002058 51.91568763  0.61002058 ...  0.61002058  0.61002058\n",
      "  51.91568763]\n",
      " [ 0.44569461 55.35143704  0.44569461 ...  0.44569461  0.44569461\n",
      "  55.35143704]\n",
      " [ 0.75720532 44.1527623   0.75720532 ...  0.75720532  0.75720532\n",
      "  44.1527623 ]\n",
      " ...\n",
      " [ 0.1681764  46.52332579  0.1681764  ...  0.1681764   0.1681764\n",
      "  46.52332579]\n",
      " [ 0.42350058 51.4038948   0.42350058 ...  0.42350058  0.42350058\n",
      "  51.4038948 ]\n",
      " [ 0.56448345 48.80009498  0.56448345 ...  0.56448345  0.56448345\n",
      "  48.80009498]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66106/1406895302.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  f = np.sum(np.log(1 + np.exp(-y_pred * y))) + lam * np.linalg.norm(w) / 2\n",
      "/tmp/ipykernel_66106/1406895302.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  g = - X.T @ (y / (1 + np.exp(y * y_pred))) + lam * w\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Logistic Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LeastSquaresVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Least Square\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = HingeLossVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Hinge Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets us code the above Loss Fuctions in CVXPY!\n",
    "\n",
    "CVXPY is an open source Python-embedded modeling language for convex optimization problems. Link: https://www.cvxpy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    \n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f1,g1] = LogisticLossCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value Naive = \" + str(f1))\n",
    "print(\"Printing Gradient Naive:\")\n",
    "print(g1)\n",
    "\n",
    "start = time.time()\n",
    "[f2,g2] = LeastSquaresCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value For = \" + str(f2))\n",
    "print(\"Printing Gradient For:\")\n",
    "print(g2)\n",
    "\n",
    "start = time.time()\n",
    "[f2,g2] = HingeLossCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value For = \" + str(f2))\n",
    "print(\"Printing Gradient For:\")\n",
    "print(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the losses with Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossFun(w, X, y, lam):\n",
    "    return error_ll\n",
    "\n",
    "def LeastSquaresFun(w, X, y, lam):\n",
    "    return error_ls\n",
    "\n",
    "def HingeLossFun(w, X, y, lam):\n",
    "    return error_hl\n",
    "\n",
    "def plot_errors(error_ll, error_ls, error_hl, num):\n",
    "    plt.plot(num, error_ll, label=\"Logistic Loss\")\n",
    "    plt.plot(num, error_ls, label=\"Least Squares\")\n",
    "    plt.plot(num, error_hl, label=\"Hinge Loss\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "error_ll = LogisticLossFun(w,X,y,1)\n",
    "error_ls = LeastSquaresFun(w,X,y,1)\n",
    "error_hl = HingeLossFun(w,X,y,1)\n",
    "plot_errors(error_ll, error_ls, error_hl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
