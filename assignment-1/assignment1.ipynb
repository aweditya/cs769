{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the random Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1 -1  1 -1 -1 -1  1  1  1 -1 -1 -1  1\n",
      " -1 -1 -1  1 -1 -1  1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1 -1  1  1  1 -1 -1\n",
      "  1 -1 -1  1 -1  1 -1 -1 -1 -1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1 -1 -1  1\n",
      "  1 -1  1  1  1 -1  1  1 -1 -1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1\n",
      " -1 -1  1 -1]\n",
      "[[0.63165729 0.56259315 0.88911601 0.99243595 0.97148293 0.42418679\n",
      "  0.9618053  0.64594619 0.71626428 0.60537136]\n",
      " [0.31332431 0.98303431 0.6052717  0.85491642 0.19914815 0.59130496\n",
      "  0.54753222 0.90284044 0.71468023 0.09258027]\n",
      " [0.45464133 0.1415053  0.55578822 0.37241692 0.81183935 0.2797956\n",
      "  0.91916133 0.6970818  0.06208752 0.32051508]\n",
      " [0.70376647 0.52942961 0.69125709 0.60365735 0.58405237 0.1239638\n",
      "  0.65152582 0.47784894 0.54981361 0.58706855]\n",
      " [0.55386265 0.84703028 0.79557686 0.96638507 0.97611291 0.77637023\n",
      "  0.07002549 0.74345025 0.68793438 0.62568656]\n",
      " [0.46695015 0.97228329 0.67228182 0.33928301 0.01464147 0.03805836\n",
      "  0.23502414 0.23625863 0.52670759 0.0498618 ]\n",
      " [0.05439027 0.93441341 0.85938234 0.1762513  0.93718592 0.94604559\n",
      "  0.38030897 0.57265522 0.02669353 0.22860317]\n",
      " [0.71079592 0.26200932 0.10624956 0.90664702 0.2054474  0.76261743\n",
      "  0.22714287 0.51566779 0.54971589 0.51224333]\n",
      " [0.89737166 0.47111847 0.55652587 0.2824924  0.86778969 0.35773895\n",
      "  0.33062731 0.69691536 0.4180345  0.185302  ]\n",
      " [0.33880502 0.04551565 0.98854113 0.43268432 0.38630453 0.21698742\n",
      "  0.10934432 0.99539077 0.66908796 0.20737542]\n",
      " [0.84360524 0.0924069  0.49737799 0.14488552 0.57502416 0.73172266\n",
      "  0.97407536 0.59336863 0.1539062  0.29808403]\n",
      " [0.70901528 0.21271528 0.34476719 0.91116043 0.82114702 0.61579795\n",
      "  0.54105898 0.82284572 0.64679897 0.05426095]\n",
      " [0.05816182 0.70672166 0.66850561 0.10324403 0.3351542  0.29399159\n",
      "  0.50824363 0.29332229 0.11178791 0.89415804]\n",
      " [0.49252351 0.99435562 0.82062309 0.85405181 0.39342911 0.34032662\n",
      "  0.20930551 0.16090308 0.8436468  0.51071261]\n",
      " [0.2986817  0.52682255 0.69169745 0.72944995 0.63304382 0.09464106\n",
      "  0.3658311  0.88980496 0.17684147 0.42622618]\n",
      " [0.94530765 0.84027662 0.98388162 0.59081381 0.08154749 0.26641634\n",
      "  0.49167063 0.99134459 0.5781143  0.0722566 ]\n",
      " [0.82920562 0.82563448 0.42651461 0.2855163  0.81817962 0.46866001\n",
      "  0.34736586 0.48546895 0.85332265 0.68880152]\n",
      " [0.44938544 0.80473965 0.67065468 0.14919415 0.43447539 0.09901518\n",
      "  0.24999357 0.51303979 0.63118989 0.20402751]\n",
      " [0.54052969 0.50218502 0.18320695 0.2797296  0.7361609  0.18869685\n",
      "  0.45118013 0.7592126  0.45045353 0.90502989]\n",
      " [0.41656619 0.27528105 0.55989676 0.21817069 0.346766   0.23192382\n",
      "  0.46673774 0.45193321 0.77843594 0.51105994]\n",
      " [0.81999201 0.63863721 0.711211   0.28705734 0.42946143 0.93058125\n",
      "  0.91755155 0.52527454 0.49574684 0.33503747]\n",
      " [0.70058198 0.78040978 0.12779083 0.47259174 0.4233268  0.97785591\n",
      "  0.5954878  0.02301613 0.29295419 0.78631738]\n",
      " [0.91115925 0.32573479 0.97581729 0.52572705 0.09972632 0.28483564\n",
      "  0.23308493 0.77619002 0.20403825 0.39758776]\n",
      " [0.89799312 0.53754304 0.56587691 0.54149664 0.4540325  0.14908736\n",
      "  0.03226368 0.15672761 0.90309989 0.17206044]\n",
      " [0.17207781 0.1409932  0.42739512 0.5415272  0.84162292 0.91850265\n",
      "  0.24449182 0.26209145 0.00989447 0.25389279]\n",
      " [0.89643523 0.21101659 0.27292182 0.10212506 0.11138485 0.23026833\n",
      "  0.89319693 0.04208367 0.52654841 0.70566656]\n",
      " [0.04665906 0.20043515 0.02643024 0.82074355 0.09284184 0.52098238\n",
      "  0.53314635 0.32475382 0.74264055 0.80731561]\n",
      " [0.48592521 0.97441283 0.28537377 0.7070851  0.74804097 0.86125409\n",
      "  0.73197893 0.68730287 0.97398783 0.70004282]\n",
      " [0.26877066 0.63583039 0.82029869 0.91674243 0.6124976  0.28462473\n",
      "  0.35444352 0.14944466 0.71168197 0.52765226]\n",
      " [0.62810664 0.33200471 0.91850928 0.9595349  0.20164717 0.41297919\n",
      "  0.91995186 0.20186746 0.66107762 0.93630788]\n",
      " [0.10152377 0.24457062 0.80838383 0.80406431 0.20141656 0.45732931\n",
      "  0.9138169  0.74795072 0.31710573 0.86182181]\n",
      " [0.52497927 0.56272096 0.21641232 0.79536189 0.17505473 0.3561345\n",
      "  0.66177421 0.01094804 0.81028595 0.49597039]\n",
      " [0.95096097 0.65413063 0.44035743 0.76648455 0.20034359 0.13105273\n",
      "  0.88917758 0.71611871 0.45908419 0.07441589]\n",
      " [0.64792676 0.23048296 0.77856485 0.2524456  0.51689611 0.07761482\n",
      "  0.69796025 0.37078614 0.72165396 0.36146134]\n",
      " [0.44701318 0.87946641 0.72968796 0.71983322 0.48225081 0.87033967\n",
      "  0.17457442 0.05211337 0.21942604 0.4834168 ]\n",
      " [0.95988364 0.18335628 0.00308072 0.69765562 0.78170822 0.24036071\n",
      "  0.16161329 0.60773663 0.43722581 0.39992445]\n",
      " [0.80838477 0.80683154 0.1372573  0.25624368 0.75699719 0.55808858\n",
      "  0.64425844 0.95451234 0.28990639 0.86146188]\n",
      " [0.49417179 0.32479367 0.89971299 0.30833271 0.07148404 0.11205867\n",
      "  0.98051157 0.01462283 0.76131996 0.71467879]\n",
      " [0.69902733 0.53629153 0.50207018 0.01392948 0.59203511 0.28912622\n",
      "  0.30507449 0.82418944 0.98588125 0.58690341]\n",
      " [0.62576476 0.12659669 0.22966573 0.90437214 0.42494691 0.52089511\n",
      "  0.98354754 0.59874645 0.91607095 0.75072864]\n",
      " [0.14106792 0.19943272 0.38092295 0.86920388 0.78646176 0.49027172\n",
      "  0.95682487 0.34104414 0.45054565 0.91941761]\n",
      " [0.16525442 0.56396049 0.94856391 0.08084561 0.44994188 0.95872213\n",
      "  0.94147528 0.51339159 0.73456478 0.85167175]\n",
      " [0.045685   0.9691365  0.44171268 0.83867276 0.80168534 0.10912855\n",
      "  0.6887597  0.44351397 0.41741552 0.595492  ]\n",
      " [0.71370194 0.1229744  0.78885038 0.95722758 0.15705195 0.4536089\n",
      "  0.38323751 0.14463668 0.55363576 0.00640947]\n",
      " [0.36876854 0.00355857 0.05791792 0.34232869 0.14329738 0.27097253\n",
      "  0.39111436 0.17266073 0.08061822 0.35672452]\n",
      " [0.18785017 0.86514679 0.79175877 0.64827371 0.1357697  0.00904317\n",
      "  0.23700663 0.25362364 0.86462497 0.23838331]\n",
      " [0.14851782 0.37462833 0.21889211 0.31733518 0.82185419 0.88923442\n",
      "  0.75455458 0.2287289  0.89618457 0.45409741]\n",
      " [0.87396462 0.41561387 0.47467431 0.47516906 0.13228652 0.58277649\n",
      "  0.64847389 0.85520176 0.30706765 0.35006193]\n",
      " [0.71254248 0.56177212 0.24076952 0.62640039 0.48558922 0.42431631\n",
      "  0.02535101 0.00521914 0.68241965 0.20060541]\n",
      " [0.44732287 0.75595629 0.66927602 0.14131564 0.14783993 0.58059021\n",
      "  0.08728718 0.25790689 0.56281493 0.97486617]\n",
      " [0.64629879 0.4383262  0.0987601  0.69898942 0.04808539 0.90090436\n",
      "  0.6990987  0.29901122 0.84465771 0.18709306]\n",
      " [0.08046866 0.6729916  0.87995396 0.52194674 0.44778554 0.12722724\n",
      "  0.3894897  0.63886605 0.59726354 0.18229927]\n",
      " [0.97649913 0.93479301 0.66890542 0.19111134 0.50988288 0.80302579\n",
      "  0.78517068 0.75627211 0.46633427 0.88661844]\n",
      " [0.49452302 0.79211516 0.02014941 0.28644272 0.60497314 0.08857813\n",
      "  0.41247253 0.54567408 0.85370259 0.86818555]\n",
      " [0.61197698 0.07205247 0.87309473 0.28884255 0.87134284 0.97153616\n",
      "  0.20793077 0.79039202 0.27826871 0.36615547]\n",
      " [0.28365263 0.40058018 0.46195737 0.22206427 0.60190805 0.81825402\n",
      "  0.62707813 0.1472861  0.23241624 0.29588224]\n",
      " [0.82619683 0.88022501 0.58459495 0.50954573 0.24345142 0.5318244\n",
      "  0.87857403 0.22789959 0.09399936 0.50819374]\n",
      " [0.59893505 0.47029346 0.37833795 0.41616725 0.40330451 0.65415058\n",
      "  0.66526732 0.37616375 0.9309404  0.50146671]\n",
      " [0.63570811 0.8306842  0.82520718 0.54190112 0.35375446 0.87349951\n",
      "  0.09523106 0.43233584 0.47931898 0.96619615]\n",
      " [0.80198248 0.39258571 0.49055068 0.44785591 0.24415395 0.4028094\n",
      "  0.83095561 0.18683139 0.94381767 0.61069674]\n",
      " [0.91915655 0.71454875 0.74896448 0.33545587 0.75400736 0.69355134\n",
      "  0.93890028 0.11587306 0.23599212 0.92239222]\n",
      " [0.43442271 0.82341316 0.79885537 0.49496779 0.01495953 0.05268881\n",
      "  0.0649762  0.84375314 0.50587985 0.68577448]\n",
      " [0.37286889 0.73472085 0.20955733 0.46504513 0.33581675 0.36263834\n",
      "  0.10648716 0.85428982 0.53918916 0.22021544]\n",
      " [0.92027289 0.49330422 0.23746966 0.85863042 0.87731193 0.84870949\n",
      "  0.44244617 0.76616975 0.89879656 0.7486157 ]\n",
      " [0.43104458 0.66517821 0.20704998 0.67888423 0.72663189 0.24232938\n",
      "  0.51939159 0.56098706 0.77255901 0.30515846]\n",
      " [0.06560687 0.97408529 0.30484168 0.53651143 0.37664943 0.33926584\n",
      "  0.99408647 0.50139981 0.40448523 0.1865943 ]\n",
      " [0.85749948 0.94005367 0.40309434 0.58923368 0.75153312 0.72948754\n",
      "  0.50904788 0.34698737 0.85767114 0.86909988]\n",
      " [0.82899602 0.03768833 0.19301259 0.60763909 0.0533379  0.96483487\n",
      "  0.28364207 0.58439582 0.72248198 0.17692819]\n",
      " [0.3159851  0.55451424 0.51208339 0.84216189 0.46414205 0.69151411\n",
      "  0.66633084 0.81766051 0.40536803 0.77798136]\n",
      " [0.02245012 0.27298284 0.8265754  0.50758167 0.96821602 0.68699732\n",
      "  0.50359022 0.39690054 0.07024168 0.82325227]\n",
      " [0.87644055 0.12160523 0.459938   0.73743249 0.57248324 0.58897822\n",
      "  0.26583735 0.1766587  0.16680979 0.24424203]\n",
      " [0.30846366 0.85913739 0.6203102  0.85893082 0.20104433 0.28009\n",
      "  0.83602637 0.01358691 0.28476329 0.85881182]\n",
      " [0.06877345 0.85346119 0.97664589 0.89631103 0.99497589 0.6471173\n",
      "  0.89401348 0.87416709 0.57722158 0.8762053 ]\n",
      " [0.91569742 0.53198923 0.21494065 0.41192679 0.99147015 0.94344414\n",
      "  0.38443267 0.04553208 0.56026981 0.36895191]\n",
      " [0.65980204 0.67636929 0.83768906 0.50475781 0.66314042 0.70428494\n",
      "  0.86045239 0.18054721 0.51260475 0.83174125]\n",
      " [0.3893033  0.14165887 0.49385525 0.72550507 0.73380444 0.64704906\n",
      "  0.04455361 0.86732148 0.49530415 0.67454125]\n",
      " [0.7146011  0.2445889  0.58515547 0.93859046 0.89210019 0.75706924\n",
      "  0.12705039 0.9328699  0.56431116 0.27380484]\n",
      " [0.48496657 0.08388136 0.72576697 0.68056411 0.72790777 0.99443142\n",
      "  0.63042986 0.76129259 0.39912138 0.6502451 ]\n",
      " [0.52144486 0.09048192 0.23299599 0.12247387 0.88474311 0.06170962\n",
      "  0.52889973 0.32047406 0.3800256  0.42712838]\n",
      " [0.75250955 0.4405718  0.92805736 0.04693822 0.34407696 0.0332299\n",
      "  0.87656174 0.72477594 0.35844629 0.28002138]\n",
      " [0.78525449 0.01335156 0.0059591  0.02954961 0.9044527  0.15420209\n",
      "  0.59613976 0.53603094 0.54790449 0.15554847]\n",
      " [0.69094824 0.54133192 0.29660557 0.67818311 0.24383149 0.88966045\n",
      "  0.45857777 0.92733879 0.79267581 0.4356116 ]\n",
      " [0.79064098 0.8250015  0.20786601 0.139566   0.05720265 0.63159001\n",
      "  0.741164   0.95375249 0.08239722 0.32451874]\n",
      " [0.40199535 0.52675096 0.28248158 0.96012577 0.34206635 0.09004269\n",
      "  0.24504348 0.99336197 0.84921071 0.0223056 ]\n",
      " [0.01679976 0.1047245  0.00393341 0.58281052 0.00565066 0.44864592\n",
      "  0.24926105 0.13334226 0.53599665 0.59640397]\n",
      " [0.18957794 0.02455466 0.31824172 0.1250469  0.03173581 0.62927287\n",
      "  0.68990487 0.91656185 0.3209801  0.6410012 ]\n",
      " [0.36424309 0.20705547 0.55122459 0.7738587  0.83577969 0.58421911\n",
      "  0.01204756 0.02301551 0.74218609 0.43169998]\n",
      " [0.38264822 0.00133775 0.91602991 0.97399834 0.42336491 0.2445237\n",
      "  0.20385248 0.72988273 0.41028539 0.243075  ]\n",
      " [0.88390677 0.91927905 0.94197335 0.94048037 0.10826847 0.99979717\n",
      "  0.34361537 0.40322331 0.66196916 0.86826739]\n",
      " [0.3638745  0.09265899 0.02119625 0.06700771 0.75087016 0.7633449\n",
      "  0.33286391 0.57863871 0.22173272 0.22031244]\n",
      " [0.38239445 0.34958828 0.29956568 0.62703945 0.7715379  0.57636727\n",
      "  0.84498592 0.02938144 0.2987342  0.4041816 ]\n",
      " [0.10396745 0.18248725 0.07134234 0.40219394 0.74072949 0.63367464\n",
      "  0.62627167 0.44682502 0.38809479 0.16826541]\n",
      " [0.3981782  0.48726887 0.79748357 0.8864456  0.01925366 0.64746037\n",
      "  0.09708517 0.11061498 0.45136963 0.50543176]\n",
      " [0.49112606 0.4848383  0.65854742 0.89991035 0.59423409 0.00860467\n",
      "  0.46947633 0.4887921  0.10669696 0.69395439]\n",
      " [0.57418753 0.24389067 0.95958549 0.94324671 0.93496847 0.87492507\n",
      "  0.06096582 0.41827435 0.95388565 0.7328321 ]\n",
      " [0.32826874 0.37180244 0.87141488 0.88298618 0.94697707 0.34461523\n",
      "  0.38047089 0.9457425  0.73624555 0.08426992]\n",
      " [0.13249188 0.76708554 0.7450399  0.33833282 0.99932694 0.60249155\n",
      "  0.64966579 0.50557256 0.10949717 0.76337765]\n",
      " [0.42502828 0.88627309 0.51987965 0.28028972 0.35892928 0.78784131\n",
      "  0.65274099 0.06849505 0.83686716 0.33510338]\n",
      " [0.98724897 0.63911939 0.10796951 0.77628188 0.00904535 0.91593436\n",
      "  0.33962756 0.35305091 0.26378342 0.51996117]\n",
      " [0.97679924 0.51141636 0.48870937 0.38119315 0.77011709 0.50640204\n",
      "  0.59212966 0.38318484 0.44263513 0.60785918]]\n"
     ]
    }
   ],
   "source": [
    "# We first generate a random dataset with number of features (m = 10) and number of instances (n = 100)\n",
    "# We also generate a random label vector y \\in {-1,1}\n",
    "\n",
    "n = 100 # Number of instances\n",
    "m = 10  # Number of Features\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n) # n-dimensional vector\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m) # m-dimensional vector\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Logistic Loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    Z = 1e3 # To prevent overflow in exp\n",
    "    eps = 1e-12 # For numerical stability in log computation\n",
    "    \n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += np.log(1 + np.exp(-y[i] * y_pred[i] / Z) + eps)\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            g[k] += -1 / Z * y[i] * X[i][k] / (1 + np.exp(y_pred[i] * y[i] / Z))\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "    \n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 3.950590133666992\n",
      "Function value = [1827.10474119]\n",
      "Printing Gradient:\n",
      "[0.11749169 0.15899    0.81681781 ... 0.99246212 0.85652192 0.366302  ]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Least Squares \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += (y[i] - y_pred[i]) ** 2\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            g[k] += 2 * (y_pred[i] - y[i]) * X[i][k]\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "        \n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 2.1725127696990967\n",
      "Function value = [6.4106057e+08]\n",
      "Printing Gradient:\n",
      "[255753.40689687 240648.72892564 254237.27395052 ... 238644.84294679\n",
      " 263388.49241077 236782.33777121]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Implementation of the Hinge Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossNaive(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    f = 0.0\n",
    "    (n, m) = X.shape\n",
    "    y_pred = np.zeros(n)\n",
    "    # Cost function\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            y_pred[i] += w[j] * X[i][j] \n",
    "            \n",
    "        f += max(0, 1 - y[i] * y_pred[i])\n",
    "        \n",
    "    for j in range(m):\n",
    "        f += (lam * w[j] * w[j]) / 2\n",
    "    \n",
    "    # Gradient\n",
    "    g = np.zeros(m)\n",
    "    for k in range(m):\n",
    "        for i in range(n):\n",
    "            if y_pred[i]*y[i] <= 1:\n",
    "                g[k] += -1 * y[i] * X[i][k]\n",
    "            \n",
    "        g[k] += lam * w[k]\n",
    "    \n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken = 2.0967795848846436\n",
      "Function value = [125883.29747759]\n",
      "Printing Gradient:\n",
      "[25.54630495 25.78453339 24.58310184 ... 22.42755837 26.88608145\n",
      " 21.21990544]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Loss\n",
      "Time Taken = 3.909250497817993\n",
      "Function value = [1789.21708714]\n",
      "Printing Gradient:\n",
      "[0.18113387 0.05562184 0.75208868 ... 0.9953454  0.28985262 0.10854626]\n",
      "Least Square\n",
      "Time Taken = 2.183762550354004\n",
      "Function value = [6.25283582e+08]\n",
      "Printing Gradient:\n",
      "[270274.16131549 249061.0909122  266718.70685752 ... 218919.31429342\n",
      " 266244.22989347 262762.69786772]\n",
      "Hinge Loss\n",
      "Time Taken = 2.0598666667938232\n",
      "Function value = [116701.93514227]\n",
      "Printing Gradient:\n",
      "[24.67905062 21.78016385 23.05061244 ... 20.77612319 23.42611931\n",
      " 25.79488864]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Logistic Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LeastSquaresNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Least Square\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = HingeLossNaive(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Hinge Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a vectorized version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    Z = 1e3 # To prevent overflow in exp\n",
    "    eps = 1e-12 # For numerical stability in log computation\n",
    "    \n",
    "    y_pred = X @ w\n",
    "    f = np.sum(np.log(1 + np.exp(-y_pred * y / Z) + eps)) + lam * np.linalg.norm(w) / 2\n",
    "    g = - X.T / Z @ (y / (1 + np.exp(y * y_pred / Z))) + lam * w\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.sum((y_pred - y) ** 2) + lam * np.linalg.norm(w) / 2\n",
    "    g = 2 * X.T @ (y_pred - y) + lam * w\n",
    "    return [f, g]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossVec(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    y_pred = X @ w\n",
    "    f = np.sum((1 >= y_pred * y) * (1 - y_pred*y)) + lam * np.linalg.norm(w) / 2\n",
    "    g = - X.T @ ((1 >= y_pred * y) * y) + lam * w\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Loss\n",
      "Time Taken = 0.013798236846923828\n",
      "Function value = 14020.012456035533\n",
      "Printing Gradient:\n",
      "[[0.34092635 0.34092635 0.34092635 ... 0.34092635 0.34092635 0.34092635]\n",
      " [0.57263392 0.57263392 0.57263392 ... 0.57263392 0.57263392 0.57263392]\n",
      " [0.81774459 0.81774459 0.81774459 ... 0.81774459 0.81774459 0.81774459]\n",
      " ...\n",
      " [0.32067814 0.32067814 0.32067814 ... 0.32067814 0.32067814 0.32067814]\n",
      " [0.46680414 0.46680414 0.46680414 ... 0.46680414 0.46680414 0.46680414]\n",
      " [0.90843705 0.90843705 0.90843705 ... 0.90843705 0.90843705 0.90843705]]\n",
      "Least Square\n",
      "Time Taken = 0.013409614562988281\n",
      "Function value = 61980869772.81122\n",
      "Printing Gradient:\n",
      "[[247537.70437758 247537.70437758 247537.70437758 ... 247537.70437758\n",
      "  247537.70437758 247537.70437758]\n",
      " [269971.68443648 269971.68443648 269971.68443648 ... 269971.68443648\n",
      "  269971.68443648 269971.68443648]\n",
      " [268148.79633246 268148.79633246 268148.79633246 ... 268148.79633246\n",
      "  268148.79633246 268148.79633246]\n",
      " ...\n",
      " [246460.20895865 246460.20895865 246460.20895865 ... 246460.20895865\n",
      "  246460.20895865 246460.20895865]\n",
      " [253613.44594057 253613.44594057 253613.44594057 ... 253613.44594057\n",
      "  253613.44594057 253613.44594057]\n",
      " [231361.87459902 231361.87459902 231361.87459902 ... 231361.87459902\n",
      "  231361.87459902 231361.87459902]]\n",
      "Hinge Loss\n",
      "Time Taken = 0.009453296661376953\n",
      "Function value = 13199603.898598736\n",
      "Printing Gradient:\n",
      "[[50.01362347 50.01362347 50.01362347 ... 50.01362347 50.01362347\n",
      "  50.01362347]\n",
      " [54.72027342 54.72027342 54.72027342 ... 54.72027342 54.72027342\n",
      "  54.72027342]\n",
      " [54.59925827 54.59925827 54.59925827 ... 54.59925827 54.59925827\n",
      "  54.59925827]\n",
      " ...\n",
      " [49.72915132 49.72915132 49.72915132 ... 49.72915132 49.72915132\n",
      "  49.72915132]\n",
      " [51.31552124 51.31552124 51.31552124 ... 51.31552124 51.31552124\n",
      "  51.31552124]\n",
      " [47.33049913 47.33049913 47.33049913 ... 47.33049913 47.33049913\n",
      "  47.33049913]]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LogisticLossVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Logistic Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = LeastSquaresVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Least Square\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)\n",
    "\n",
    "start = time.time()\n",
    "[f,g] = HingeLossVec(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Hinge Loss\")\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value = \" + str(f))\n",
    "print(\"Printing Gradient:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets us code the above Loss Fuctions in CVXPY!\n",
    "\n",
    "CVXPY is an open source Python-embedded modeling language for convex optimization problems. Link: https://www.cvxpy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    Z = 1e3 # To prevent overflow in exp\n",
    "    eps = 1e-12 # For numerical stability in log computation\n",
    "    \n",
    "    expression = cp.sum(cp.log(1 + cp.exp(cp.multiply(-y, X @ w / Z)) + eps)) + lam * cp.norm(w, 2) / 2\n",
    "    Problem = cp.Problem(cp.Minimize(expression))\n",
    "    f = expression.value\n",
    "    g = - X.T / Z @ (y / (1 + np.exp(y * y_pred / Z))) + lam * w\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquaresCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    expression = cp.sum_squares(X @ w - y) + lam * cp.norm(w, 2) / 2\n",
    "    f = expression.value\n",
    "    g = 2 * X.T @ (y_pred - y) + lam * w\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HingeLossCVXPY(w, X, y, lam):\n",
    "    # Computes the cost function for all the training samples\n",
    "    # where f is the function value and g is the gradient\n",
    "    expression = cp.sum(cp.pos(1 - cp.multiply(y, X @ w))) + lam * cp.norm(w, 2) / 2\n",
    "    Problem = cp.Problem(cp.Minimize(expression))\n",
    "    f = expression.value\n",
    "    return [f, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "m = 10\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "start = time.time()\n",
    "[f1,g1] = LogisticLossCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value Naive = \" + str(f1))\n",
    "print(\"Printing Gradient Naive:\")\n",
    "print(g1)\n",
    "\n",
    "start = time.time()\n",
    "[f2,g2] = LeastSquaresCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value For = \" + str(f2))\n",
    "print(\"Printing Gradient For:\")\n",
    "print(g2)\n",
    "\n",
    "start = time.time()\n",
    "[f2,g2] = HingeLossCVXPY(w,X,y,1)\n",
    "end = time.time()\n",
    "print(\"Time Taken = \" + str(end - start))\n",
    "print(\"Function value For = \" + str(f2))\n",
    "print(\"Printing Gradient For:\")\n",
    "print(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the losses with Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticLossFun(w, X, y, lam):\n",
    "    return error_ll\n",
    "\n",
    "def LeastSquaresFun(w, X, y, lam):\n",
    "    return error_ls\n",
    "\n",
    "def HingeLossFun(w, X, y, lam):\n",
    "    return error_hl\n",
    "\n",
    "def plot_errors(error_ll, error_ls, error_hl, num):\n",
    "    plt.plot(num, error_ll, label=\"Logistic Loss\")\n",
    "    plt.plot(num, error_ls, label=\"Least Squares\")\n",
    "    plt.plot(num, error_hl, label=\"Hinge Loss\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 10000\n",
    "\n",
    "X = np.random.rand(n,m)\n",
    "y = np.random.rand(n)\n",
    "ybin = [(int(yi >= 0.5) - int(yi < 0.5)) for yi in y]\n",
    "y = np.array(ybin)\n",
    "w = np.random.rand(m, 1)\n",
    "\n",
    "error_ll = LogisticLossFun(w,X,y,1)\n",
    "error_ls = LeastSquaresFun(w,X,y,1)\n",
    "error_hl = HingeLossFun(w,X,y,1)\n",
    "plot_errors(error_ll, error_ls, error_hl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
